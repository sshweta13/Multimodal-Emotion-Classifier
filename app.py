# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ylvmJcmXHbeIF3agpa_TPa2YIno65-Nw
"""

import gradio as gr
import torch
from transformers import BertTokenizer, BertForSequenceClassification
from deepface import DeepFace
import cv2
import tempfile
import matplotlib.pyplot as plt
import numpy as np
import random

# Emotion labels and tips
label_dict = {
    'happy': 0, 'sad': 1, 'angry': 2,
    'surprise': 3, 'disgust': 4, 'not-relevant': 5
}
label_dict_inverse = {v: k for k, v in label_dict.items()}
mood_tips = {
    "happy": ["Keep spreading joy!", "Celebrate your wins!", "Smile at someone today."],
    "sad": ["Listen to uplifting music.", "Take a short walk.", "Write 3 things you're grateful for."],
    "angry": ["Try deep breathing.", "Write your thoughts.", "Take a short break from the screen."],
    "surprise": ["Share your surprise!", "Explore what amazed you.", "Reflect on the moment."],
    "disgust": ["Step away for a moment.", "Talk to someone.", "Note what triggered it."],
    "not-relevant": ["Stay neutral.", "Observe without judgment.", "Take a breath."]
}

# Load BERT model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_dict))
bert_model.load_state_dict(torch.load('BERT_ft_epoch1.model', map_location='cpu'))
bert_model.eval()

# Plot bar chart
def plot_emotions_bar(emotions):
    fig, ax = plt.subplots(figsize=(6, 3))
    labels = list(emotions.keys())
    values = list(emotions.values())
    bars = ax.bar(labels, values, color=plt.cm.Set3(range(len(labels))))
    ax.set_ylabel('Confidence')
    ax.set_title('Facial Emotion Confidence')
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height:.2f}', xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3), textcoords="offset points", ha='center', fontsize=8)
    plt.xticks(rotation=30)
    plt.tight_layout()
    return fig

# Predict from text
def predict_text_emotion(text):
    inputs = tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=256)
    with torch.no_grad():
        logits = bert_model(**inputs).logits
        probs = torch.nn.functional.softmax(logits, dim=1)
        pred_idx = torch.argmax(probs).item()
        emotion = label_dict_inverse[pred_idx]
        probs_dict = {label_dict_inverse[i]: float(probs[0][i]) for i in range(len(probs[0]))}
        return emotion, probs_dict

# Predict from face image
def predict_face_emotion(image):
    if image is None:
        return "No image", {}
    with tempfile.NamedTemporaryFile(suffix=".jpg", delete=False) as tmpfile:
        cv2.imwrite(tmpfile.name, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))
        result = DeepFace.analyze(tmpfile.name, actions=['emotion'], enforce_detection=False)
        dominant = result[0]['dominant_emotion']
        all_scores = result[0]['emotion']
        return dominant, all_scores

# Combined logic
def predict_emotion(text, image):
    response = ""
    tips = ""
    fig = None

    if text and text.strip():
        text_emotion, text_probs = predict_text_emotion(text)
        tip = random.choice(mood_tips.get(text_emotion, ["Take care!"]))
        response += f"<h3 style='color:#2e8b57;'>üìù You seem to be feeling <b>{text_emotion.upper()}</b></h3>"
        tips += f"<p><i>üí° Tip:</i> {tip}</p>"

    if image is not None:
        face_emotion, face_scores = predict_face_emotion(image)
        tip = random.choice(mood_tips.get(face_emotion, ["Take care!"]))
        response += f"<h3 style='color:#4682b4;'>üì∏ Your facial expression shows <b>{face_emotion.upper()}</b></h3>"
        tips += f"<p><i>üí° Tip:</i> {tip}</p>"
        fig = plot_emotions_bar(face_scores)

    if not response:
        response = "‚ö†Ô∏è Please provide text or an image."

    return response + tips, fig

# UI
demo = gr.Interface(
    fn=predict_emotion,
    inputs=[
        gr.Textbox(lines=2, placeholder="Type how you feel...", label="Text Input"),
        gr.Image(source="webcam", tool="editor", type="numpy", label="Capture or Upload Face")
    ],
    outputs=[
        gr.HTML(label="Emotion Summary"),
        gr.Plot(label="Face Emotion Chart")
    ],
    title="üåü Multimodal Emotion Classifier (Text + Webcam)",
    description="Detect your emotion from text or facial expression. Powered by BERT + DeepFace. Now with webcam capture! üé•",
    theme="soft"
)

demo.launch()